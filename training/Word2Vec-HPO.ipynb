{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimizations\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "3. [Training parameters for HPO](#Training-parameters-for-HPO)\n",
    "4. [Plot training and validation accuracies](#Plot-training-and-validation-accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***\n",
    "\n",
    "Welcome to our end-to-end example of hyperparameter tuning with blazing text (word2vec) algorithm. In this demo, we will use the HPO feature of sagemaker and train 2 models with learning rate, mini batch size and optimizer chosen by the bayesian method.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "***\n",
    "### Permissions and environment variables\n",
    "\n",
    "Before launching this notebooks, please start notebooks #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user='user1'\n",
    "my_bucket='marc-stationf-sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket=my_bucket # customize to your bucket\n",
    "\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/blazingtext:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/blazingtext:latest'}\n",
    "\n",
    "training_image = containers[boto3.Session().region_name]\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters for HPO\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Hyperparameters\n",
    "\n",
    "First we define the static hyperparameters used for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"batch_skipgram\"\n",
    "epochs = 5\n",
    "min_count = 5\n",
    "sampling_threshold = 0.0001\n",
    "#learning_rate = 0.05\n",
    "window_size = 5\n",
    "vector_dim = 100\n",
    "negative_samples = 5\n",
    "#batch_size = 11 #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "evaluation = True# Perform similarity evaluation on WS-353 dataset at the end of training\n",
    "subwords = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameters used for HPO\n",
    "\n",
    "Now we are going to define the hyperparameters tuned with the bayesian strategy : \n",
    "\n",
    "* Learning Rate\n",
    "* batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tuning_job_prefix_name = 'blazin-hpo-' + user\n",
    "timestamp = time.strftime('-%H-%M-%S', time.gmtime())\n",
    "tuning_job_name = tuning_job_prefix_name + timestamp\n",
    "\n",
    "print (tuning_job_name)\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"0.01\",\n",
    "          \"MinValue\": \"0.005\",\n",
    "          \"Name\": \"learning_rate\",\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"32\",\n",
    "          \"MinValue\": \"8\",\n",
    "          \"Name\": \"batch_size\",\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 2,\n",
    "      \"MaxParallelTrainingJobs\": 2\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"train:mean_rho\",\n",
    "      \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Params\n",
    "\n",
    "Now we create the training params for sagemaker training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# create unique job name \n",
    "job_name_prefix = 'blazin-training-' + user\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "training_params = \\\n",
    "{\n",
    "    # specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 2,\n",
    "        \"InstanceType\": \"ml.c4.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"mode\": mode,\n",
    "        \"epochs\": str(epochs),\n",
    "        \"min_count\": str(min_count),\n",
    "        \"sampling_threshold\": str(sampling_threshold),\n",
    "        \"window_size\": str(window_size),\n",
    "        \"vector_dim\": str(vector_dim),\n",
    "        \"negative_samples\": str(negative_samples),\n",
    "        \"evaluation\": str(evaluation),\n",
    "        \"subwords\": str(subwords)      \n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 360000\n",
    "    },\n",
    "#Training data should be inside a subdirectory called \"train\"\n",
    "#Validation data should be inside a subdirectory called \"validation\"\n",
    "#The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\":'s3://sagemaker-eu-west-1-542104878797/sagemaker/DEMO-blazingtext-text8/train',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Job for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "#sagemaker.create_training_job(**training_params)\n",
    "\n",
    "sagemaker.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info = sagemaker.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "status = training_info['HyperParameterTuningJobStatus']\n",
    "print(\"Training job ended with status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('logs')\n",
    "\n",
    "lgn='/aws/sagemaker/TrainingJobs'\n",
    "\n",
    "response = client.describe_log_streams(\n",
    "    logGroupName=lgn,\n",
    "    logStreamNamePrefix='blazin-hpo-' + user,\n",
    "    orderBy='LogStreamName',\n",
    "    descending=True,\n",
    "    limit=50\n",
    ")\n",
    "logstreams = response['logStreams']\n",
    "\n",
    "response = sagemaker.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "hpoName = response['HyperParameterTuningJobName']\n",
    "\n",
    "for logstream in logstreams:\n",
    "    if hpoName in logstream['logStreamName']:\n",
    "        print(logstream['logStreamName'])\n",
    "        logN = client.get_log_events(logGroupName=lgn, logStreamName=logstream['logStreamName'])\n",
    "        events = logN['events']\n",
    "        for event in events:\n",
    "            if '#mean_rho' in event['message']:\n",
    "                print(event['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Training Job Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "Status = response['HyperParameterTuningJobStatus']\n",
    "BTJ = response['BestTrainingJob']\n",
    "\n",
    "print(\"HPO Status : \" + Status)\n",
    "btjName = BTJ['TrainingJobName']\n",
    "print(btjName)\n",
    "for key, value in BTJ['TunedHyperParameters'].items():\n",
    "    print(key + \" : \" + value)\n",
    "print(\"RHO Mean : \" + str(BTJ['FinalHyperParameterTuningJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 path to access Best training job model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('s3://{}/{}/output/{}/output/model.tar.gz'.format(bucket, job_name_prefix,btjName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
